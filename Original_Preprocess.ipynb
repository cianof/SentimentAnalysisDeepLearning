{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Downloading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle Set Up - Note this is only applicable for Google Colab\n",
    "Skip if wish to use data from local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_tok = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ~/.kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp kaggle.json ~/.kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kaggle datasets list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download -d kosweet/cleaned-emotion-extraction-dataset-from-twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip cleaned-emotion-extraction-dataset-from-twitter.zip -d dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beginning of Notebook once downloading above complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D, Bidirectional, LSTM, Flatten, Dense, Input, Dropout, MaxPooling1D, Concatenate\n",
    "from tensorflow.keras.layers import SpatialDropout1D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from datetime import datetime\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_decision = int(input('Enter 1 to read from local machine or 2 from Kaggle'))\n",
    "if user_decision == 1:\n",
    "    path = 'C:/Users/cferr/Documents/4th Year/DL_Data/dataset(clean).csv'\n",
    "    #path to glove download file\n",
    "    glove_path = 'C:/Users/cferr/Documents/4th Year/DL_Data/'\n",
    "else:\n",
    "    path = '/content/dataset/dataset(clean).csv'\n",
    "    glove_path=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path, encoding = \"ISO-8859-1\")\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(user_decision==2):\n",
    "    !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "    !unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=purple> Exploratory Data Analysis / Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the first things I'm noticing is the 'Content' column does some work in getting the original tweet cleaned up.<br>It removes accounts tagged with '@', it appears to remove all occurences of commas and apostrophes, it removes attached links and finally it alters the use of emojis to make them into single words e.g. 'facewithtearsofjoy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data = df.isna().sum().sort_values(ascending=False)\n",
    "percentage_missing = round((df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)*100,2)\n",
    "missing_info = pd.concat([missing_data,percentage_missing],keys=['Missing values','Percentage'],axis=1)\n",
    "missing_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is a good sign, it is an indication that we ae dealing with a complete dataset and do not have to worry about dealing with empty columns or anything of the sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df.groupby(by='Emotion').agg('count')\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualise this a little better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "counts['Content'].plot(kind='bar', subplots=True, figsize=(10, 8))\n",
    "plt.title(\"Pie chart of different classes of tweets\",fontsize=16)\n",
    "plt.ylabel(\"\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "counts['Content'].plot(kind='pie', subplots=True, figsize=(10, 8), autopct='%1.1f%%')\n",
    "plt.title(\"Pie chart of different classes of tweets\",fontsize=16)\n",
    "plt.ylabel(\"\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see clearly that there is a very even split between the different emotion classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Original Content'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = df.Content.str.split().apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into testing and training sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per the assignment specifications the data should be shuffled with random seed=0, take last 20% of data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.2, random_state=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train dataset shape: {}'.format(train.shape))\n",
    "print('Test dataset shape: {}'.format(test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "documents = [_text.split() for _text in df.Content] \n",
    "print(f'Time Taken: {round(time.time()-t)} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(train.Content)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "print(\"Vocabulary Size :\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=tokenizer.texts_to_sequences(train.Content)\n",
    "x_test=tokenizer.texts_to_sequences(test.Content)\n",
    "pad_size = max(len(x) for x in x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network needs to recieve inputs of a stable length. Given that tweets can range from 1-280 characters means there is quite a bit of variation in the length of our tweets, we therefore must apply something called padding, this is the process of lenghtening each tweets vector representation by the addition of zeros, these zeros mean that the data itself is not compromised but allows for a consistent input into the network.<br> keras employ a useful tool for such an action called pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "x_train = pad_sequences(x_train,\n",
    "                        maxlen = pad_size, padding='post')\n",
    "x_test = pad_sequences(x_test,\n",
    "                       maxlen =pad_size,padding='post')\n",
    "\n",
    "print(\"Training X Shape:\",x_train.shape)\n",
    "print(\"Testing X Shape:\",x_test.shape)\n",
    "print(f'Time Taken: {round(time.time()-t)} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=counts.index.unique()\n",
    "labels=labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "\n",
    "y_train = encoder.fit_transform(train.Emotion.to_list())\n",
    "y_test = encoder.fit_transform(test.Emotion.to_list())\n",
    "\n",
    "y_train = y_train.reshape(-1,1)\n",
    "y_test = y_test.reshape(-1,1)\n",
    "\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using GloVe - Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove(glove_file):\n",
    "  with open(glove_file, 'r', encoding=\"utf8\") as f:\n",
    "    words = set()\n",
    "    word_to_vec_map = {}\n",
    "\n",
    "    for line in f:\n",
    "      line = line.strip().split()\n",
    "      word = line[0]\n",
    "      words.add(word)\n",
    "      vec = line[1:]\n",
    "      word_to_vec_map[word] = np.array(vec, dtype=np.float64)\n",
    "\n",
    "  return  word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_map = read_glove(glove_path+'glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "  if word in word_map:\n",
    "    embedding_matrix[i] = word_map[word]\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are dealing with public twitter data there is a high likelyhood of misspellings therefore we can see from the below that 42% of our embedding matrix is going to remain filled with zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
    "print(int(100*nonzero_elements / vocab_size),'%')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
    "                                          embedding_dim,\n",
    "                                          weights=[embedding_matrix],\n",
    "                                          input_length=pad_size,\n",
    "                                          trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=pad_size, dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_sequences = embedding_layer(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shared Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE =128\n",
    "EPOCHS =3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A reproducement of Kim Yoons CNN for sentence classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_list = []\n",
    "filter_sizes = [3,8]\n",
    "\n",
    "for filt in filter_sizes:\n",
    "    x = Conv1D(128, filt, activation='relu',padding='same')(embedded_sequences)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    conv_list.append(x)\n",
    "    \n",
    "x = Concatenate(axis=-1)(conv_list)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(3, activation='sigmoid')(x)\n",
    "\n",
    "model=Model(inp,output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "ReduceLROnPlateau = ReduceLROnPlateau(factor=0.1,\n",
    "                                     min_lr = 0.01,\n",
    "                                     monitor = 'val_loss',\n",
    "                                     verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_cnn = model.fit(x_train, y_train, batch_size=128, epochs=3,\n",
    "                    validation_split=0.1, callbacks=[ReduceLROnPlateau])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(datetime.now().strftime(\"%d_%m_%Y__%H_%M\")+\"_CNN\"+\".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting(history, name):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "#     lr =history.history['lr']\n",
    "    epochs = range(len(acc))\n",
    "\n",
    "    plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy for ' + name)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss for '+ name)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting(history_cnn, 'CNN Orig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn =load_model('18_04_2021__14_29_CNN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model_cnn.evaluate(x_test, y_test, verbose = 1) \n",
    "\n",
    "print('Test loss:', score[0]) \n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM - CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=pad_size, dtype='int32')\n",
    "embedded_sequences = embedding_layer(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTM(32,dropout=0.2, return_sequences=True, recurrent_dropout=0.2)(embedded_sequences)\n",
    "\n",
    "conv_list =[]\n",
    "filter_sizes=[3,8]\n",
    "\n",
    "for filt in filter_sizes:\n",
    "    conv = Conv1D(128, filt, activation='relu')(lstm)\n",
    "    conv = MaxPooling1D(pool_size=2)(conv)\n",
    "    conv_list.append(conv)\n",
    "    \n",
    "lstm_conv = Concatenate(axis=1)(conv_list) \n",
    "lstm_conv = Flatten()(lstm_conv)\n",
    "lstm_conv = Dense(128, activation='relu')(lstm_conv)\n",
    "lstm_conv = Dropout(0.5)(lstm_conv)\n",
    "output = Dense(3, activation='sigmoid')(lstm_conv)\n",
    "model_soa=Model(inp,output)\n",
    "model_soa.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_soa.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_lstmcnn = model_soa.fit(x_train, y_train, batch_size=128, epochs=3,\n",
    "                    validation_split=0.1, callbacks=[ReduceLROnPlateau])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting(history_lstmcnn, 'LSTM-CNN Orig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_soa.save(datetime.now().strftime(\"%d_%m_%Y__%H_%M\")+\"_LSTM_CNN\"+\".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model_soa.evaluate(x_test, y_test, verbose = 1) \n",
    "\n",
    "print('Test loss:', score[0]) \n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn=load_model('18_04_2021__14_29_CNN.h5')\n",
    "model_lstm_cnn=load_model('22_04_2021__16_01_LSTM_CNN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('CNN With Original Pre-Process:')\n",
    "score_cnn = model_cnn.evaluate(x_test, y_test, verbose = 1)\n",
    "print('LSTM-CNN With Original Pre-Process:')\n",
    "score_lstm_cnn = model_lstm_cnn.evaluate(x_test, y_test, verbose = 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Comparison Across 4 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CNN_ORIG'] = score_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LSTM_CNN_ORIG'] = score_lstm_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(index = {0:'Loss', 1:'Accuracy'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Test_Results_orig.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig = pd.read_csv('Test_Results_orig.csv')\n",
    "df_pre = pd.read_csv('Test_Results_pre.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig.drop('Unnamed: 0',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre.drop('Unnamed: 0',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupedBarAccuracyData = np.array([[\"CNN\", \"Preprocessed\", df_pre['CNN_Pre'][1]],\n",
    "                                [\"CNN\",\"Original\",df_orig['CNN_ORIG'][1]], \n",
    "                                [\"LSTM_CNN\", \"Original\", df_orig['LSTM_CNN_ORIG'][1]],\n",
    "                                [\"LSTM_CNN\", \"Preprocessed\", df_pre['LSTM_CNN_PRE'][1]]\n",
    "                               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupedBarLossData = np.array([[\"CNN\", \"Preprocessed\", df_pre['CNN_Pre'][0]],\n",
    "                                [\"CNN\",\"Original\",df_orig['CNN_ORIG'][0]], \n",
    "                                [\"LSTM_CNN\", \"Original\", df_orig['LSTM_CNN_ORIG'][0]],\n",
    "                                [\"LSTM_CNN\", \"Preprocessed\", df_pre['LSTM_CNN_PRE'][0]]\n",
    "                               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupedBarAccuracyDataFrame = pd.DataFrame(groupedBarAccuracyData, columns = [\"Model\", \"Data\", \"Accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupedBarLossDataFrame = pd.DataFrame(groupedBarLossData, columns = [\"Model\", \"Data\", \"Loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "g = sns.catplot(x=\"Data\", y=\"Loss\", hue=\"Model\", data=groupedBarLossDataFrame,\n",
    "                height=6, kind=\"bar\", palette=\"bright\")\n",
    "g.despine(left=True)\n",
    "g.set_ylabels(\"Loss\\n\", fontsize = 16)\n",
    "g.set_xlabels(\"Dataset\", fontsize = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupedBarAccuracyDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "g = sns.catplot(x=\"Data\", y=\"Accuracy\", hue=\"Model\", data=groupedBarAccuracyDataFrame,\n",
    "                height=6, kind=\"bar\", palette=\"bright\")\n",
    "g.despine(left=True)\n",
    "g.set_ylabels(\"Accuracy\\n\", fontsize = 16)\n",
    "g.set_xlabels(\"Dataset\", fontsize = 16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
